#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Music Mood Classification Using The Million Song Dataset
\end_layout

\begin_layout Date
Bhavika Tekwani
\end_layout

\begin_layout Date
December 12, 2016
\end_layout

\begin_layout Abstract
In this paper, music mood classification is tackled from an audio signal
 analysis perspective.
 There's an increasing volume of digital content available every day.
 To make this content discoverable and accesible, there's a need for better
 techniques that automatically analyze this content.
 Here, we present a summary of techniques that can be used to classify music
 as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 through audio content analysis.
 The paper shows that timbral and spectral features like MFCC can indeed
 be used for mood classification with a fair degree of success.
 We also compare the effects of using certain descriptive features like
 acousticness, speechiness, danceability and instrumentalness for this type
 of binary mood classification as against combining them with timbral and
 spectral features.
 We find that the models we use for classification rate danceability, energy,
 speechiness and the number of beats as important features as compared to
 others during the classification task.
 This correlates to the way most humans interpret music as happy or sad.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Music Mood Classification is a task within music information retrieval (MIR)
 that is frequently addressed by performing sentiment analysis on song lyrics.
 The approach in this paper aims to explore to what degree audio features
 extracted from audio analysis tools like 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{librosa}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{pyAudioAnalysis}
\end_layout

\end_inset

 and others aid a binary classification task.
 This task has an appreciable level of complexity because of the inherent
 subjectivity in the way people interpret music.
 We believe that despite this subjectivity, there are patterns to be found
 in a song that could help place it on 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textbf{Russell's}
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

 2D representation of valence and arousal.
 Audio features might be able to overcome some of the limitations of lyrics
 analysis when the music we aim to classify is instrumental or when the
 song spans many different genres.
 Mood classification has applications ranging from rich metadata extraction
 to recommender systems.
 A mood component added to metadata would make for better indexing and search
 techniques leading to better discoverability of music for use in films
 and television shows.
 Music applications that enable algorithmic playlist generation based on
 mood would make for richer, user-centric applications.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
REDO THIS : In the next few chapters, we discuss the Million Song Dataset,
 an evaluation of the different techniques tried for mood classification
 and how they stack up against previous work done in this area.
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Problem Statement
\end_layout

\begin_layout Standard
We aim to achieve the best possible accuracy in classifying our subset of
 songs as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 For the sake of simplicity, we limit ourselves to these two labels though
 they're an oversimplification of the complex emotional nature of music.
 
\end_layout

\begin_layout Subsection*
2.1 Notations
\end_layout

\begin_layout Section
Literature Review
\end_layout

\begin_layout Section
Methods and Techniques
\end_layout

\begin_layout Section
Discussion and Results
\end_layout

\begin_layout Subsection
Datasets
\end_layout

\begin_layout Subsection
Evaluation Metrics
\end_layout

\begin_layout Subsection
Experimental Results
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Subsection
Directions for Future Work
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

J.
 A.
 Russell, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{A Circumplex Model of Effect}
\end_layout

\end_inset

, Journal of Personality and Social Psychology, (6), 1980.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

 Thierry Bertin-Mahieux, Daniel P.W.
 Ellis, Brian Whitman, and Paul Lamere.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{The Million Song Dataset}
\end_layout

\end_inset

.
 In Proceedings of the 12th International Society for Music Information
 Retrieval Conference (ISMIR 2011), 2011
\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
