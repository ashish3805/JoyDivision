#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{caption}
\usepackage{tabulary}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip 2bp
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Music Mood Classification Using The Million Song Dataset
\end_layout

\begin_layout Author
Bhavika Tekwani
\end_layout

\begin_layout Date
December 12, 2016
\end_layout

\begin_layout Abstract
In this paper, music mood classification is tackled from an audio signal
 analysis perspective.
 There's an increasing volume of digital content available every day.
 To make this content discoverable and accesible, there's a need for better
 techniques that automatically analyze this content.
 Here, we present a summary of techniques that can be used to classify music
 as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 through audio content analysis.
 The paper shows that low level audio features like MFCC can indeed be used
 for mood classification with a fair degree of success.
 We also compare the effects of using certain descriptive features like
 acousticness, speechiness, danceability and instrumentalness for this type
 of binary mood classification as against combining them with timbral and
 pitch features.
 We find that the models we use for classification rate danceability, energy,
 speechiness and the number of beats as important features as compared to
 others during the classification task.
 This correlates to the way most humans interpret music as happy or sad.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Music Mood Classification is a task within music information retrieval (MIR)
 that is frequently addressed by performing sentiment analysis on song lyrics.
 The approach in this paper aims to explore to what degree audio features
 extracted from audio analysis tools like 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{librosa}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{pyAudioAnalysis}
\end_layout

\end_inset

 and others aid a binary classification task.
 This task has an appreciable level of complexity because of the inherent
 subjectivity in the way people interpret music.
 We believe that despite this subjectivity, there are patterns to be found
 in a song that could help place it on Russell's 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

 2D representation of valence and arousal.
 Audio features might be able to overcome some of the limitations of lyrics
 analysis when the music we aim to classify is instrumental or when the
 song spans many different genres.
 Mood classification has applications ranging from rich metadata extraction
 to recommender systems.
 A mood component added to metadata would make for better indexing and search
 techniques leading to better discoverability of music for use in films
 and television shows.
 Music applications that enable algorithmic playlist generation based on
 mood would make for richer, user-centric applications.
 In the next few chapters, we discuss the approach that leads us to 75%
 accuracy and how it compares to other work done in this area.
\end_layout

\begin_layout Section
Problem Statement
\end_layout

\begin_layout Standard
We aim to achieve the best possible accuracy in classifying our subset of
 songs as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 For the sake of simplicity, we limit ourselves to these two labels though
 they do not sufficiently represent the complex emotional nature of music.
 
\end_layout

\begin_layout Subsection*
2.1 Notations
\end_layout

\begin_layout Standard
We introduce some notations for the feature representations in this paper.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation}
\end_layout

\begin_layout Plain Layout


\backslash
mathit{f_{timbre_avg}} = [timavg_{1}, timavg_{2}...timavg_{12}]
\end_layout

\begin_layout Plain Layout


\backslash
end{equation}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(1) represents the vector of timbral average features at the song level.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation}
\end_layout

\begin_layout Plain Layout


\backslash
mathit{f_{pitch}} = [pitch_{1}, pitch_{2}...pitch_{12}]
\end_layout

\begin_layout Plain Layout


\backslash
end{equation}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(2) represents a vector of chroma average features at the song level.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation}
\end_layout

\begin_layout Plain Layout


\backslash
mathit{f_{timbre}} = [tim_{1}, tim_{2}...tim_{90}]
\end_layout

\begin_layout Plain Layout


\backslash
end{equation}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(3) is a vector of mean and covariance values of all the segments aggregated
 at the song level.
\end_layout

\begin_layout Section
\noindent
Literature Review
\end_layout

\begin_layout Subsection
Automatic Mood Detection and Tracking of Music Audio Signals (Lie Lu et
 al) 
\end_layout

\begin_layout Standard
Lie Lu et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5"

\end_inset

 explore a hierarchical framework for classifying music into four mood clusters.
 Working with a dataset of 250 pieces of classical music, they extract timbral
 Mel Frequency Cepstral Coefficients (MFCC) and define spectral features
 like shape and contrast.
 These are used in the form of a 25 dimensional timbre feature.
 Rhythm features are extracted at the song level by finding the onset curve
 of each subband (an octave based section of a 32 ms frame) and summing
 them.
 Calculating average correlation peak, ratio between average peak strength
 and average valley strength, average tempo and average onset frequency
 leads to a five element rhythm feature vector.
 They use the mean and standard deviation of the frame level features (timbre
 and intensity) to capture the overall structure of the frame.
 
\end_layout

\begin_layout Standard
A Gaussian Mixture Model (GMM) with 16 mixtures is used to model each feature
 related to a particular mood cluster.
 The Expectation Maximization (EM) algorithm is used to estimate the parameters
 of Gaussian components and mixture weights.
 In this case, K-Means is used for initialization.
 Once the GMM models are obtained, the mood classification depends on a
 simple hypothesis test with the intensity features given by the equation
 below.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation}
\end_layout

\begin_layout Plain Layout

 
\backslash
lambda =     
\end_layout

\begin_layout Plain Layout

    
\backslash
frac{P(G_{1}/I)}{P(G_{2}/I)}, 
\end_layout

\begin_layout Plain Layout

    
\backslash
left
\backslash
{
\backslash
begin{array}{@{}lr@{}}     
\end_layout

\begin_layout Plain Layout

    
\backslash
geq 1, 
\backslash
text{Select & G_{1}}  
\backslash

\backslash
     < 1 ,  
\backslash
text{Select & G_{2}}                           
\end_layout

\begin_layout Plain Layout

    
\backslash
end{array}
\end_layout

\begin_layout Plain Layout


\backslash
end{equation}
\end_layout

\begin_layout Plain Layout


\backslash
end{array} 
\backslash
right 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\lambda$
\end_inset

 represents the likelihood ratio, G
\begin_inset script subscript

\begin_layout Plain Layout
i
\end_layout

\end_inset

 represents different mood groups, I is the intensity feature set and P(G
\begin_inset script subscript

\begin_layout Plain Layout
i
\end_layout

\end_inset

| I) is the probability that a particular audio clip belongs to a mood group
 G
\begin_inset script subscript

\begin_layout Plain Layout
i 
\end_layout

\end_inset

given its Intensity features which are calculated from the GMM.
 
\end_layout

\begin_layout Subsection
Aggregate Features and ADABOOST for Music Classification (Bergstra et al)
\end_layout

\begin_layout Standard
Bergstra et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-12"

\end_inset

 present a solution towards artist and genre recognition.
 Their technique employs frame compression to convert frames from songs
 to a song level set of features based on covariance.
 They borrow from West & Cox 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"

\end_inset

 who introduce a 
\begin_inset Quotes eld
\end_inset

memory feature
\begin_inset Quotes erd
\end_inset

 containing the mean and variance of a frame.
 After computing frames, they group non-overlapping blocks of frames into
 segments.
 Segment summarization is done by fitting independent Gaussian models to
 the features.
 Covariance between the features is ignored.
 The resulting mean and variance values are inputs to ADABOOST.
 Bergstra et all explore the effects of varying segment lengths on classificatio
n accuracy and conclude that in smaller segments, mean and variance of the
 segments have higher variance.
\end_layout

\begin_layout Subsection
An Exploration of Mood Classification in the Million Songs Dataset (Corona
 et al)
\end_layout

\begin_layout Standard
Corona et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"

\end_inset

 perform mood classification on the Million Song Dataset using lyrics as
 features.
 They experiment with term weighting schemes like TF, TF-IDF, Delta TF-IDF
 and BM25 to explore the term distributions across four mood quadrants defined
 by Russell
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

.
 The Kruskal Wallis test is used to measure statistically significant difference
s in the results obtained using different term weighting schemes.
 They find that a support vector machine (SVM) provides the best accuracy
 and moods like 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{angst}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{rage}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{cool-down}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{depressive}
\end_layout

\end_inset

 were predicted with higher accuracy than others.
 
\end_layout

\begin_layout Subsection
Music Mood Classification
\end_layout

\begin_layout Standard
Goel & Padial 
\begin_inset CommandInset citation
LatexCommand cite
key "key-11"

\end_inset

 attempt binary classification for mood on the Million Song Dataset.
 They use features like Tempo, Energy, Mode, Key and Harmony.
 The harmony feature is engineered as a 7 element vector.
 A soft margin SVM with the RBF kernel is used for classification to provide
 a success rate of 75.76%.
\end_layout

\begin_layout Subsection
Music Genre Classification with the Million Song Dataset
\end_layout

\begin_layout Standard
Liang et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-7"

\end_inset

 use a blend model for music genre classification with feature classes comprisin
g of Hidden Markov Model (HMM) genre probabilities extracted from timbre
 features, loudness and tempo, lyrics bag-of-words submodel probabilities
 and emotional valence.
 They assume each genre corresponds to one HMM and use labeled training
 data to train one HMM for each genre.
 Additionally, they combine audio and textual (lyrics) features for Canonical
 Correlation Analysis (CCA) by revealing shared linear correlations between
 audio and lyrics features in order to design a low dimensional, shared
 feature representation.
 
\end_layout

\begin_layout Section
Methods and Techniques
\end_layout

\begin_layout Subsection
Feature Engineering and Selection
\end_layout

\begin_layout Standard
For mood classification, one of the questions we try to answer is, can a
 model capture the attributes that make a song 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 the same way we as humans do? To answer this question, we used Recursive
 Feature Elimination (RFECV) with a Random Forest Classifier and 5-fold
 cross validation.
 Recursive Feature Elimination is a Backwards Selection technique that helps
 you find the optimal number of features that minimize the training error.
 Additionally, once we select the features we also examine the relative
 importance of these features for different estimators to better understand
 whether some features are better indicators of mood than others.
 We mutliplied mode and key and tempo and mode to capture the multiplicative
 relations between these features.
 Loudness is provided in decibels and is often negative, so we squared the
 value for better interpretability.
 Values for Speechiness, Danceability, Energy, Acousticness and Instrumentalness
 were often missing when we tried using the Spotify API to fetch them.
 In that case, we imputed the mean of these values.
 
\begin_inset VSpace 2pt
\end_inset


\end_layout

\begin_layout Standard
The dataset includes two features Segments Pitches and Segments Timbre which
 are both 2D arrays of varying shapes.
 A segment is a 0.3 second long frame in a song.
 This means that the number of segments varies with the song.
 Segments Timbre is a 12 dimensional MFCC-like feature for every segment.
 MFCC is a representation of the short-term power spectrum of a sound obtained
 by taking a cosine transform of the power spectrum and converting it to
 the Mel scale.
 These are very commonly used in audio analysis for speech recognition tasks.
 In our dataset, the Echo Nest API's Analyze documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

 states that they provide Segments Timbre functions by extracting MFCC for
 each segment in a song and then using Principal Component Analysis (PCA)
 to compactly represent them as a 12 element vector.
 In a similar vein, Segments Pitches represent the chroma features of each
 segment in a 12 dimensional vector.
 Here, the 12 elements of the vector represent pitch classes like C, C#,
 B and so on.
 The challenge is - to find a uniform representation of timbre and pitches
 that represents a whole song.
 We use a technique called segment aggregation
\begin_inset CommandInset citation
LatexCommand cite
key "key-10,key-12,key-5"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset VSpace 2pt
\end_inset


\end_layout

\begin_layout Standard
Segment aggregation involves computing several statistical moments like
 mean, minimum, maximum, standard deviation, kurtosis, variances and covariances
 across each segment.
 We try two methods.
 First, we compute a vector containing the mean and covariances of all segments
 and obtain a 90 element vector (12 averages and 78 covariances).
 We can use this approach for timbre and pitch arrays both.
 The drawback is that 90 elements make for a very large feature vector and
 they would need to be pruned in some way or the most important elements
 would have to be identified.
 Using PCA is not desirable here for two reasons: timbre features have already
 been extracted through PCA on MFCC values and our segment aggregation does
 not account for temporal relations between the segments.
 This leads to some loss of information in the segments.
 Using the 90 element vectors as they are introduces the curse of dimensionality.
 Our second approach is calculating only the elementwise mean of all segments
 in a song.
 This gives us two 12 dimensional vectors for pitches and timbre.
 Now, we use these as features for our models.
 
\begin_inset VSpace 2pt
\end_inset


\end_layout

\begin_layout Standard
Using RFECV, we selected 12 timbre features (equation (1)), 12 pitch averages
 (equation (2)) and descriptive features like Danceability, Speechiness,
 Beats, LoudnessSq, Instrumentalness, Energy and Acousticness for a total
 of 31 features.
 Other features like Key*Mode, Tempo*Mode, Time Signature, Key and Mode
 were found to not aid the classification task and were discarded.
 
\end_layout

\begin_layout Subsection
Classification Models
\end_layout

\begin_layout Standard
For this binary classification problem, we evaluate several models and compare
 how the perform on the test set.
 To tune the performance of each model, we perform a hyperparameter search
 and then select the ones that perform best with each model.
 5 fold cross validation is used during the hyperparameter search.
 
\end_layout

\begin_layout Standard
Table 1 below shows the different estimators we used and the parameters
 we tuned for each.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]   
\backslash
centering 
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{|l|l|}     
\backslash
hline     
\backslash
textbf{Estimators}   & 
\backslash
textbf{Hyperparameters}                                                
                     
\backslash

\backslash
 
\backslash
hline     Random Forest Classifier       & estimators= 300, max.
 depth = 15     
\end_layout

\begin_layout Plain Layout


\backslash

\backslash
 
\backslash
hline     XGBoost Classifier    & max.
 depth = 5,   max.
 delta step = 0.1   
\end_layout

\begin_layout Plain Layout

 
\backslash

\backslash
 
\backslash
hline     Gradient Boosting Classifier   & loss = exponential, max.
 depth = 6, criteria = mse, estimators = 200 
\backslash

\backslash
 
\backslash
hline     ADABOOST Classifier            & learning rate = 0.1, no.
 of estimators = 300    
\end_layout

\begin_layout Plain Layout


\backslash

\backslash
 
\backslash
hline     Extra Trees Classifier         & max.
 depth = 15, estimators = 100    
\end_layout

\begin_layout Plain Layout


\backslash

\backslash
 
\backslash
hline     SVM   & C = 2, kernel = linear, gamma = 0.1  
\end_layout

\begin_layout Plain Layout


\backslash

\backslash
 
\backslash
hline     Gaussian Naive Bayes           & priors = None  
\end_layout

\begin_layout Plain Layout

 
\backslash

\backslash
 
\backslash
hline     K Nearest Neighbour Classifier & number of neighbours = 29, P
 = 2, metric = euclidean     
\end_layout

\begin_layout Plain Layout

 
\backslash

\backslash
 
\backslash
hline     
\backslash
end{tabular}     
\backslash
caption {Tuned hyperparameters for various estimators} 
\backslash
end{table}
\end_layout

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Discussion and Results
\end_layout

\begin_layout Subsection
Datasets
\end_layout

\begin_layout Standard
We are using the Million Song Dataset (MSD) created by LabROSA at Columbia
 University in association with Echo Nest.
 The dataset contains audio features and metadata for a million popular
 tracks.
 For the purpose of this project, we use the subset of 10,000 songs made
 available by LabROSA.
 The compressed file containing this subset is 1.8 GB in size.
 Using this dataset in its original form was a challenging task.
 We hand labeled 7396 songs as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 This was time consuming and the only hurdle to attempting hierarchical
 classification.
 We use a naive definition of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 labels.
 Songs that would be interpreted as angry, depressing, melancholic, wistful,
 brooding, tense/anxious have all been tagged as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 On the other hand songs interpreted as joyful, rousing, confident, fun,
 cheerful, humourous, silly have been tagged as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

.
 Admittedly, this is an oversimplification of the ways music can be analyzed
 and understood.
 An obvious caveat of this method is that it does not account for subjectivity
 in the labels and only one frame of reference is used as ground truth.
 However, to deal with this to some extent, we dropped songs that we couldn't
 neatly bucket into either labels.
 This means that a song as complex as Queen's Bohemian Rhapsody does not
 appear in the dataset.
 In Table 1, we present a snapshot of the data available to us and Table
 2 shows the different categories our attributes fall into.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]   
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{|l|l|}     
\backslash
hline     
\backslash
textbf{Million Song Dataset} & 
\backslash
textbf{Spotify API}      
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
hline     Artist Name          & Danceability     
\backslash

\backslash
 
\backslash
hline     Title                & Speechiness      
\backslash

\backslash
 
\backslash
hline     Tempo      
\end_layout

\begin_layout Plain Layout

         & Instrumentalness 
\backslash

\backslash
 
\backslash
hline     Loudness             & Energy           
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
hline     Segment Pitches      & Acousticness     
\backslash

\backslash
 
\backslash
hline     Segment Timbre      
\end_layout

\begin_layout Plain Layout

& ~                
\backslash

\backslash
 
\backslash
hline     Beats confidence     & ~                
\backslash

\backslash
 
\backslash
hline     Loudness (dB)   
\end_layout

\begin_layout Plain Layout

    & ~                
\backslash

\backslash
 
\backslash
hline     Duration (seconds)   & ~                
\backslash

\backslash
 
\backslash
hline     Mode                 & ~              
\end_layout

\begin_layout Plain Layout

 
\backslash

\backslash
 
\backslash
hline     Key                  & ~                
\backslash

\backslash
 
\backslash
hline     Time Signature       & ~               
\end_layout

\begin_layout Plain Layout


\backslash

\backslash
 
\backslash
hline    
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
caption {Fields in the Million Song Dataset and Spotify API} 
\backslash
end{table}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{|l|l|l|}  
\end_layout

\begin_layout Plain Layout


\backslash
hline     
\backslash
textbf{Notational} & 
\backslash
textbf{Descriptive}                          & 
\backslash
textbf{Audio}                  
\backslash

\backslash
 
\backslash
hline     Key, Mode,          & Speechiness, Danceability, Instrumentalness,
  & Segment Pitches, Segment Timbre 
\backslash

\backslash
 
\backslash
hline     Time Signature      & Energy, Acousticness                   
       & Tempo, Beats confidence         
\backslash

\backslash
 
\backslash
hline     
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
caption {Attribute Categories} 
\end_layout

\begin_layout Plain Layout


\backslash
end{table} 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
On downloading the dataset and inspecting it, we found that the values of
 Energy and Danceability which were supposed to be a part of the dataset
 were 0 in all the tracks.
 According to the Analyze documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

, it means these values were not analyzed.
 However, Energy and Danceability were crucial features that we needed for
 our task.
 To solve this problem, we used the Spotify API (the Echo Nest API is now
 a part of Spotify's Web API).
 We fetched descriptive features like Energy, Acousticness, Danceability,
 Instrumentalness and Speechiness for the 7396 songs.
\end_layout

\begin_layout Subsection
Evaluation Metrics
\end_layout

\begin_layout Standard
The dataset contains a near equal distribution of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 songs as shown in Table 4.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]  
\backslash
centering   
\backslash
begin{tabular}{|l|l|l|}     
\backslash
hline     
\backslash
textbf{Label} & 
\backslash
textbf{Train} & 
\backslash
textbf{Test} 
\backslash

\backslash
 
\backslash
hline     Happy & 2171  & 1522 
\backslash

\backslash
 
\backslash
hline     Sad   & 2205  & 1498 
\backslash

\backslash
 
\backslash
hline     
\backslash
end{tabular} 
\backslash
caption {Train and test set distributions} 
\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Hence, we decide that Accuracy would be the correct metric to use.
 
\end_layout

\begin_layout Standard
Accuracy is defined in (5) where TP, TN, FP, FN stand for True Positive,
 True Negative, False Positive and False Negative.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation}
\end_layout

\begin_layout Plain Layout


\backslash
text{Accuracy} = 
\backslash
frac{TP+TN}{TP+TN+FP+FN}
\end_layout

\begin_layout Plain Layout


\backslash
end{equation}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experimental Results
\end_layout

\begin_layout Standard
We aim to evaluate three types of feature subsets.
 
\end_layout

\begin_layout Standard
In Table 5, P represents Pitch, T represents Timbre and D represents Descriptive
 features.
 
\end_layout

\begin_layout Standard
Timbre and pitch features are shown in equations (1) and (2) respectively.
 
\end_layout

\begin_layout Standard
Descriptive features include Danceability, Energy, Speechiness, Acousticness,
 Instrumentalness, Beats and LoudnessSq.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]  
\backslash
centering   
\backslash
begin{tabular}{|l|l|l|}     
\backslash
hline     
\backslash
textbf{Estimator}                       & 
\backslash
textbf{Features} & 
\backslash
textbf{Test Accuracy} 
\backslash

\backslash
 
\backslash
hline     Random Forest Classifier        & P, T, D  & 0.7456        
\backslash

\backslash
 
\backslash
hline     ~                               & P , T    & 0.7291        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.7182        
\backslash

\backslash
 
\backslash
hline     ADABOOST Classifier             & P.
 T.
 D  & 0.7354        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.7168        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.7119        
\backslash

\backslash
 
\backslash
hline     XGBoost Classifier              & P, T, D  & 
\backslash
textbf{0.7533}        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.7344        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.7165        
\backslash

\backslash
 
\backslash
hline     Gradient Boosting Classifier    & P, T, D  & 
\backslash
textbf{0.7552}        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.7145        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.7105        
\backslash

\backslash
 
\backslash
hline     SVM                             & P, T, D  & 0.7350        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.7142        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.6966        
\backslash

\backslash
 
\backslash
hline     K Nearest Neighbor Classifier   & P, T, D  & 0.6397        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.6725        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.5360        
\backslash

\backslash
 
\backslash
hline     Extra Trees Classifier          & P, T, D  & 0.7447        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.7245        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.7178        
\backslash

\backslash
 
\backslash
hline     Gaussian Naive Bayes Classifier & P, T, D  & 0.6821        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.6645        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.6417        
\backslash

\backslash
 
\backslash
hline     Voting Classifier               & P, T, D  & 
\backslash
textbf{0.7506}        
\backslash

\backslash
 
\backslash
hline     ~                               & P, T     & 0.7238        
\backslash

\backslash
 
\backslash
hline     ~                               & D        & 0.7132        
\backslash

\backslash
 
\backslash
hline     
\backslash
end{tabular}     
\backslash
caption {Classification Accuracy by Estimator and Features} 
\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
We observe from our experimental results that Ensemble classifiers like
 Random Forests, XGBoost, Gradient Boosting Classifier, ADABOOST perform
 better on our test set than SVMs and Naive Bayes classifier.
 Comparing our results to the work of Goel & Padial 
\begin_inset CommandInset citation
LatexCommand cite
key "key-11"

\end_inset

 we see that our highest accuracy is 75.52 % with a Gradient Boosting Classifier
 whereas they achieved 75.76% with an SVM using an RBF kernel.
 The difference in dataset size is significant as we compare our 7396 to
 their 233.
 We feel that this is a fair result but the feature extraction process can
 be improved.
 To answer the questions we ask in the problem statement, yes, audio features
 do aid in the mood classification task.
 Table 5 shows that using audio features like pitch and timbre along with
 descriptive features provides atleast a 3% increase in accuracy.
 Additionally, pitch and timbre averages themselves are sufficient to reach
 72.91% accuracy with Random Forest Classifiers.
 
\end_layout

\begin_layout Subsection
Directions for Future Work
\end_layout

\begin_layout Standard
In this music mood classification task, the lack of ground truth labels
 for a dataset as large as MSD was a significant hurdle to any further explorati
on of genre-mood relationships, canonical correlation analysis between music
 and lyrics or hierarchical mood classification.
 We attempted some analysis to understand the relation between genre and
 mood but we only had genre labels for approximately 2000 songs out of the
 7396 we labeled.
 Now that we are able to achieve upto 75% test accuracy, hierarchical mood
 classification would be the next step if we had ground truth labels for
 moods that fall under 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 We can demonstrate this
\begin_inset space ~
\end_inset

by building a recommender system that allows you to enter a song title and
 then suggests a song similar to the one you entered.
 Similarity of songs would be based on features like emotional valence,
 timbre, pitch and others.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
A simple framework for this would have the following steps:
\end_layout

\begin_layout Standard
1) Enter a song title based on which you want recommendations.
\end_layout

\begin_layout Standard
2) Analyse the song to assign it to a mood based cluster.
\end_layout

\begin_layout Standard
3) Suggest a song from the cluster that is most similar to the one entered
 by the user based on how close they are in terms of pitch, timbre, energy
 and valence.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

J.
 A.
 Russell, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{A Circumplex Model of Effect}
\end_layout

\end_inset

, Journal of Personality and Social Psychology, (6), 1980.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset

Thierry Bertin-Mahieux, Daniel P.W.
 Ellis, Brian Whitman, and Paul Lamere.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{The Million Song Dataset}
\end_layout

\end_inset

.
 In Proceedings of the 12th International Society for Music Information
 Retrieval Conference (ISMIR 2011), 2011
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"

\end_inset

Lie Lu, D.
 Liu, and Hong-Jiang Zhang.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Automatic Mood Detection and Tracking of Music Audio Signals}
\end_layout

\end_inset

, IEEE Transactions on Audio, Speech and Language Processing 14, no.
 1 (January 2006): 5–18.
 doi:10.1109/TSA.2005.860344.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

Panagakis, Ioannis, Emmanouil Benetos, and Constantine Kotropoulos.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Music Genre Classification: A Multilinear Approach}
\end_layout

\end_inset

.
 In ISMIR, 583–588, 2008.
 http://openaccess.city.ac.uk/2109/.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"

\end_inset

Liang, Dawen, Haijie Gu, and Brendan O’Connor.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Music Genre Classification with the Million Song Dataset}
\end_layout

\end_inset

.
 Machine Learning Department, CMU, 2011.
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.2701&rep=rep1&type=pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"

\end_inset

Laurier, Cyril, Jens Grivolla, and Perfecto Herrera.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Multimodal Music Mood Classification Using Audio and Lyrics}
\end_layout

\end_inset

.
 In Machine Learning and Applications, 2008.
 ICMLA’08.
 Seventh International Conference on, 688–693.
 IEEE, 2008.
 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4725050.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-9"

\end_inset

Schindler, Alexander, and Andreas Rauber.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Capturing the Temporal Domain in Echonest Features for Improved Classific
ation Effectiveness}
\end_layout

\end_inset

.
 In International Workshop on Adaptive Multimedia Retrieval, 214–227.
 Springer, 2012.
 http://link.springer.com/chapter/10.1007/978-3-319-12093-5_13.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-10"

\end_inset

West, Kristopher, and Stephen Cox.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Features and Classifiers for the Automatic Classification of Musical
 Audio Signals}
\end_layout

\end_inset

.
 In ISMIR.
 Citeseer, 2004.
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.443.5612&rep=rep1&type=pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"

\end_inset

Padial, Jose, and Ashish Goel.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Music Mood Classification}
\end_layout

\end_inset

.
 Accessed December 16, 2016.
 http://cs229.stanford.edu/proj2011/GoelPadial-MusicMoodClassification.pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-12"

\end_inset

Bergstra, James, Norman Casagrande, Dumitru Erhan, Douglas Eck, and Balázs
 Kégl.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Aggregate Features and ADABOOST for Music Classification}
\end_layout

\end_inset

.
 Machine Learning 65, no.
 2–3 (December 2006): 473–84.
 doi:10.1007/s10994-006-9019-7.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-13"

\end_inset

Corona, Humberto, and Michael P.
 O’Mahony.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{An Exploration of Mood Classification in the Million Songs Dataset}
\end_layout

\end_inset

.
 In 12th Sound and Music Computing Conference, Maynooth University, Ireland,
 26 July-1 August 2015.
 Music Technology Research Group, Department of Computer Science, Maynooth
 University, 2015.
 http://researchrepository.ucd.ie/handle/10197/7234.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-14"

\end_inset

Dolhansky, Brian.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Musical Ensemble Classification Using Universal Background Model
 Adaptation and the Million Song Dataset}
\end_layout

\end_inset

.
 Citeseer, 2012.
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.658.4162&rep=rep1&type=pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Tristan Jehan, David DesRoches, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Echo Nest API: Analyze Documentation}
\end_layout

\end_inset

, http://developer.echonest.com/docs/v4/_static/AnalyzeDocumentation.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Ellis, Daniel PW.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Classifying Music Audio with Timbral and Chroma Features}
\end_layout

\end_inset

, In ISMIR, 7:339–340, 2007.
 https://www.ee.columbia.edu/~dpwe/pubs/Ellis07-timbrechroma.pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

 Juan Pablo Bello, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Low level features and timbre}
\end_layout

\end_inset

, New York University, http://www.nyu.edu/classes/bello/MIR_files/timbre.pdf
\end_layout

\end_body
\end_document
