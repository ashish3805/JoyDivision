#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{caption}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Music Mood Classification Using The Million Song Dataset
\end_layout

\begin_layout Date
Bhavika Tekwani
\end_layout

\begin_layout Date
December 12, 2016
\end_layout

\begin_layout Abstract
In this paper, music mood classification is tackled from an audio signal
 analysis perspective.
 There's an increasing volume of digital content available every day.
 To make this content discoverable and accesible, there's a need for better
 techniques that automatically analyze this content.
 Here, we present a summary of techniques that can be used to classify music
 as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 through audio content analysis.
 The paper shows that timbral and spectral features like MFCC can indeed
 be used for mood classification with a fair degree of success.
 We also compare the effects of using certain descriptive features like
 acousticness, speechiness, danceability and instrumentalness for this type
 of binary mood classification as against combining them with timbral and
 spectral features.
 We find that the models we use for classification rate danceability, energy,
 speechiness and the number of beats as important features as compared to
 others during the classification task.
 This correlates to the way most humans interpret music as happy or sad.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Music Mood Classification is a task within music information retrieval (MIR)
 that is frequently addressed by performing sentiment analysis on song lyrics.
 The approach in this paper aims to explore to what degree audio features
 extracted from audio analysis tools like 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{librosa}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{pyAudioAnalysis}
\end_layout

\end_inset

 and others aid a binary classification task.
 This task has an appreciable level of complexity because of the inherent
 subjectivity in the way people interpret music.
 We believe that despite this subjectivity, there are patterns to be found
 in a song that could help place it on 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textbf{Russell's}
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

 2D representation of valence and arousal.
 Audio features might be able to overcome some of the limitations of lyrics
 analysis when the music we aim to classify is instrumental or when the
 song spans many different genres.
 Mood classification has applications ranging from rich metadata extraction
 to recommender systems.
 A mood component added to metadata would make for better indexing and search
 techniques leading to better discoverability of music for use in films
 and television shows.
 Music applications that enable algorithmic playlist generation based on
 mood would make for richer, user-centric applications.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
REDO THIS : In the next few chapters, we discuss the Million Song Dataset,
 an evaluation of the different techniques tried for mood classification
 and how they stack up against previous work done in this area.
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Problem Statement
\end_layout

\begin_layout Standard
We aim to achieve the best possible accuracy in classifying our subset of
 songs as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 For the sake of simplicity, we limit ourselves to these two labels though
 they're an oversimplification of the complex emotional nature of music.
 
\end_layout

\begin_layout Subsection*
2.1 Notations
\end_layout

\begin_layout Section
Literature Review
\end_layout

\begin_layout Subsection
Automatic Mood Detection and Tracking of Music Audio Signals (Lie Lu et
 al) 
\end_layout

\begin_layout Standard
Lie Lu et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5"

\end_inset

 explore a hierarchical framework for classifying music into four mood clusters.
 Working with a dataset of 250 pieces of classical music, they extract timbral
 Mel Frequency Cepstral Coefficients (MFCC) and define spectral features
 like shape and contrast.
 These are used in the form of a 25 dimensional timbre feature.
 Rhythm features are extracted at the song level by finding the onset curve
 of each subband (an octave based section of a 32 ms frame) and summing
 them.
 Calculating average correlation peak, ratio between average peak strength
 and average valley strength, average tempo and average onset frequency
 leads to a five feature rhythm feature vector.
 They use the mean and standard deviation of the frame level features (timbre
 and intensity) to capture the overall structure of the frame.
 
\end_layout

\begin_layout Standard
A Gaussian Mixture Model (GMM) with 16 mixtures is used to model each feature
 related to a particular mood cluster.
 The Expectation Maximization (EM) algorithm is used to estimate the parameters
 of Gaussian components and mixture weights.
 K-Means is used for initialization.
 Once the GMM models are obtained, the mood classification depends on a
 simple hypothesis test with the intensity features given by (1).
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation} 
\end_layout

\begin_layout Plain Layout

 
\backslash
lambda =     
\end_layout

\begin_layout Plain Layout

    
\backslash
frac{P(G_{1}/I)}{P(G_{2}/I)}, 
\backslash
left
\backslash
{
\backslash
begin{array}{@{}lr@{}}     
\end_layout

\begin_layout Plain Layout

    
\backslash
geq 1, 
\backslash
text { Select & G_{1} } 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

     < 1 ,  
\backslash
text { Select & G_{2} }
\end_layout

\begin_layout Plain Layout

    
\backslash
end{array}
\end_layout

\begin_layout Plain Layout


\backslash
end{equation} 
\end_layout

\begin_layout Plain Layout


\backslash
end{array} 
\backslash
right 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
lambda
\end_layout

\end_inset

 represents the likelihood ratio, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
text{G_{i}}
\end_layout

\end_inset

 represents different mood groups, I is the intensity feature set and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
text{P(G_{i}|I}
\end_layout

\end_inset

 is the probability that a particular audio clip belongs to a mood group
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
text{G_{i}}
\end_layout

\end_inset

 given its Intensity features which are calculated from the GMM.
 
\end_layout

\begin_layout Subsection
Aggregate Features and ADABOOST for Music Classification (Bergstra et al)
\end_layout

\begin_layout Standard
Bergstra et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-12"

\end_inset

 present a solution towards artist and genre recognition.
 Their technique employs frame compression to convert frames from songs
 to a song level set of features based on covariance.
 They borrow from West & Cox 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"

\end_inset

 who introduce a 
\begin_inset Quotes eld
\end_inset

memory feature
\begin_inset Quotes erd
\end_inset

 containing the mean and variance of a frame.
 After computing frames, they group non-overlapping blocks of frames into
 segments.
 Segment summarization is done by fitting independent Gaussian models to
 the features.
 Covariance between the features is ignored.
 The resulting mean and variance values are inputs to ADABOOST.
 Bergstra et all explore the effects of varying segment lengths on classificatio
n accuracy and conclude that in smaller segments, mean and variance have
 higher variance.
\end_layout

\begin_layout Subsection
An Exploration of Mood Classification in the Million Songs Dataset (Corona
 et al)
\end_layout

\begin_layout Standard
Corona et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"

\end_inset

 perform mood classification on the Million Song Dataset using lyrics as
 features.
 They experiment with term weighting schemes like TF, TF-IDF, Delta TF-IDF
 and BM25 to explore the term distributions across four mood quadrants defined
 by Russell
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

.
 The Kruskal Wallis test is used to measure statistically significant difference
s in the results obtained using different term weighting schemes.
 They find that a support vector machine (SVM) provides the best accuracy
 and moods like 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{angst}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{rage}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{cool-down}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{depressive}
\end_layout

\end_inset

 were predicted with higher accuracy than others.
 
\end_layout

\begin_layout Subsection
Music Mood Classification
\end_layout

\begin_layout Standard
Goel & Padial 
\begin_inset CommandInset citation
LatexCommand cite
key "key-11"

\end_inset

 attempt binary classification for mood on the Million Song Dataset.
 They use features like Tempo, Energy, Mode, Key and Harmony.
 The harmony feature is engineered as a 7 element vector.
 A soft margin SVM with the RBF kernel is used for classification to provide
 a success rate of 75.76%.
\end_layout

\begin_layout Subsection
Music Genre Classification with the Million Song Dataset
\end_layout

\begin_layout Standard
Liang et al 
\begin_inset CommandInset citation
LatexCommand cite
key "key-7"

\end_inset

 use a blend model for music genre classification with feature classes comprisin
g of Hidden Markov Model (HMM) genre probabilities extracted from timbre
 features, loudness and tempo, lyrics bag-of-words submodel probabilities
 and emotional valence.
 They assume each genre corresponds to a HMM and use labeled training data
 to train one HMM for each genre.
 Additionally, they combine audio and textual (lyrics) features for Canonical
 Correlation Analysis (CCA) by revealing shared linear correlations between
 audio and lyrics features in order to design a low dimensional, shared
 feature representation.
 
\end_layout

\begin_layout Section
Methods and Techniques
\end_layout

\begin_layout Subsection
Feature Engineering and Selection
\end_layout

\begin_layout Subsection
Modeling
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Discussion and Results
\end_layout

\begin_layout Subsection
Datasets
\end_layout

\begin_layout Standard
We are using the Million Song Dataset (MSD) created by LabROSA at Columbia
 University in association with Echo Nest.
 The dataset contains audio features and metadata for a million popular
 tracks.
 For the purpose of this project, we use the subset of 10,000 songs made
 available by LabROSA.
 The compressed file containing this subset is 1.8 GB in size.
 Using this dataset in its original form was a challenging task.
 We hand labeled 7396 songs as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 This was very time consuming and the only limitation to taking mood classificat
ion a step further and attempting hierarchical classification.
 We use a naive definition of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

 labels.
 Songs that would be interpreted as angry, depressing, melancholic, wistful,
 brooding, tense/anxious have all been tagged as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{sad}
\end_layout

\end_inset

.
 On the other hand songs interpreted as joyful, rousing, confident, fun,
 cheerful, humourous, silly have been tagged as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{happy}
\end_layout

\end_inset

.
 Admittedly, this is an oversimplification of the ways music can be analyzed
 and understood.
 An obvious caveat of this method is that it does not account for subjectivity
 in the labels and only one frame of reference is used as ground truth.
 However, to deal with this to some extent, we dropped songs that we couldn't
 neatly bucket into either labels.
 This means that a song as complex as Queen's Bohemian Rhapsody does not
 appear in the dataset.
 In Table 1, we present a snapshot of the data available to us.
 
\end_layout

\begin_layout Standard

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[h]   
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{|l|l|}     
\backslash
hline     
\backslash
textbf{Million Song Dataset} & 
\backslash
textbf{Spotify API}      
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
hline     Artist Name          & Danceability     
\backslash

\backslash
 
\backslash
hline     Title                & Speechiness      
\backslash

\backslash
 
\backslash
hline     Tempo      
\end_layout

\begin_layout Plain Layout

         & Instrumentalness 
\backslash

\backslash
 
\backslash
hline     Loudness             & Energy           
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
hline     Segment Pitches      & Acousticness     
\backslash

\backslash
 
\backslash
hline     Segment Timbre      
\end_layout

\begin_layout Plain Layout

& ~                
\backslash

\backslash
 
\backslash
hline     Beats confidence     & ~                
\backslash

\backslash
 
\backslash
hline     Loudness (dB)   
\end_layout

\begin_layout Plain Layout

    & ~                
\backslash

\backslash
 
\backslash
hline     Duration (seconds)   & ~                
\backslash

\backslash
 
\backslash
hline     Mode                 & ~              
\end_layout

\begin_layout Plain Layout

 
\backslash

\backslash
 
\backslash
hline     Key                  & ~                
\backslash

\backslash
 
\backslash
hline     Time Signature       & ~               
\end_layout

\begin_layout Plain Layout


\backslash

\backslash
 
\backslash
hline    
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
caption {Fields in the Million Song Dataset and Spotify API} 
\backslash
end{table}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[h]
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{|l|l|l|}  
\end_layout

\begin_layout Plain Layout


\backslash
hline     
\backslash
textbf{Notational Features} & 
\backslash
textbf{Descriptive Features}                          & 
\backslash
textbf{Audio Features}                  
\backslash

\backslash
 
\backslash
hline     Key, Mode,          & Speechiness, Danceability, Instrumentalness,
  & Segment Pitches, Segment Timbre 
\backslash

\backslash
 
\backslash
hline     Time Signature      & Energy, Acousticness                   
       & Tempo, Beats confidence         
\backslash

\backslash
 
\backslash
hline     
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
caption {Feature Categories} 
\end_layout

\begin_layout Plain Layout


\backslash
end{table} 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Evaluation Metrics
\end_layout

\begin_layout Subsection
Experimental Results
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Subsection
Directions for Future Work
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

J.
 A.
 Russell, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{A Circumplex Model of Effect}
\end_layout

\end_inset

, Journal of Personality and Social Psychology, (6), 1980.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset

Thierry Bertin-Mahieux, Daniel P.W.
 Ellis, Brian Whitman, and Paul Lamere.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{The Million Song Dataset}
\end_layout

\end_inset

.
 In Proceedings of the 12th International Society for Music Information
 Retrieval Conference (ISMIR 2011), 2011
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"

\end_inset

Lie Lu, D.
 Liu, and Hong-Jiang Zhang.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Automatic Mood Detection and Tracking of Music Audio Signals}
\end_layout

\end_inset

, IEEE Transactions on Audio, Speech and Language Processing 14, no.
 1 (January 2006): 5–18.
 doi:10.1109/TSA.2005.860344.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

Panagakis, Ioannis, Emmanouil Benetos, and Constantine Kotropoulos.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Music Genre Classification: A Multilinear Approach}
\end_layout

\end_inset

.
 In ISMIR, 583–588, 2008.
 http://openaccess.city.ac.uk/2109/.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"

\end_inset

Liang, Dawen, Haijie Gu, and Brendan O’Connor.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Music Genre Classification with the Million Song Dataset}
\end_layout

\end_inset

.
 Machine Learning Department, CMU, 2011.
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.2701&rep=rep1&type=pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"

\end_inset

Laurier, Cyril, Jens Grivolla, and Perfecto Herrera.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Multimodal Music Mood Classification Using Audio and Lyrics}
\end_layout

\end_inset

.
 In Machine Learning and Applications, 2008.
 ICMLA’08.
 Seventh International Conference on, 688–693.
 IEEE, 2008.
 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4725050.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-9"

\end_inset

Schindler, Alexander, and Andreas Rauber.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Capturing the Temporal Domain in Echonest Features for Improved Classific
ation Effectiveness}
\end_layout

\end_inset

.
 In International Workshop on Adaptive Multimedia Retrieval, 214–227.
 Springer, 2012.
 http://link.springer.com/chapter/10.1007/978-3-319-12093-5_13.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-10"

\end_inset

West, Kristopher, and Stephen Cox.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Features and Classifiers for the Automatic Classification of Musical
 Audio Signals}
\end_layout

\end_inset

.
 In ISMIR.
 Citeseer, 2004.
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.443.5612&rep=rep1&type=pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"

\end_inset

Padial, Jose, and Ashish Goel.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Music Mood Classification}
\end_layout

\end_inset

.
 Accessed December 16, 2016.
 http://cs229.stanford.edu/proj2011/GoelPadial-MusicMoodClassification.pdf.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-12"

\end_inset

Bergstra, James, Norman Casagrande, Dumitru Erhan, Douglas Eck, and Balázs
 Kégl.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{Aggregate Features and ADABOOST for Music Classification}
\end_layout

\end_inset

.
 Machine Learning 65, no.
 2–3 (December 2006): 473–84.
 doi:10.1007/s10994-006-9019-7.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-13"

\end_inset

Corona, Humberto, and Michael P.
 O’Mahony.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
textit{An Exploration of Mood Classification in the Million Songs Dataset}
\end_layout

\end_inset

.
 In 12th Sound and Music Computing Conference, Maynooth University, Ireland,
 26 July-1 August 2015.
 Music Technology Research Group, Department of Computer Science, Maynooth
 University, 2015.
 http://researchrepository.ucd.ie/handle/10197/7234.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-14"

\end_inset

Dolhansky, Brian.
 “Musical Ensemble Classification Using Universal Background Model Adaptation
 and the Million Song Dataset.” Citeseer, 2012.
 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.658.4162&rep=rep1&type=pdf.
 
\end_layout

\end_body
\end_document
